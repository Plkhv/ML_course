### Кластеризация

#### Цель работы

Освоить на практике основные приемы работы с самыми распространенными алгоритмами кластеризации.

#### Задания для выполнения

1. Загрузите прилагающийся датасет. Проверьте его на чистоту, подготовьте к моделированию.
1. Разбейте датасет на три кластера методом К-средних. Оцените качество кластеризации по метрике WCSS.
1. Визуализируйте результат кластеризации. Выведите центры кластеров.
1. Выберите оптимальное количество кластеров методом локтя. Визуализируйте получившуюся кластеризацию.
1. Постройте кластеризацию с 8 кластерами. Как располагаются их центры и что это значит?
1. Нормализуйте датасет и повторите кластеризацию.
1. Постройте для наглядности кластеризацию только по двум признакам.
1. Постройте на том же датасете иерархическую кластеризацию.
1. Постройте на этих же данных кластеризацию методом DBSCAN. 

#### Методические указания

Алгоритмы кластеризации, то есть разбиения точек выборки на пересекающиеся группы по схожести значения признаков, применяются для решения многих практических задач. Самая распространенная их них - это сегментация клиентов. Задача сегментации состоит в том, чтобы выделить среди клиентов организации какие-то заранее не известные группы таким образом, чтобы внутри каждой группы оказались похожие клиенты.

Посмотрим, как работают самые базовые алгоритмы кластеризации на примере как раз такой задачи. Скачаем датасет с данными о нескольких сотнях клиентов торговой компании:

```py
df = pd.read_csv('mall_customers_clustering.csv', index_col=0)
df.head()
```

В этом наборе данных приведена базовая информация о клиентах, которая может быть полезна для категоризации их потребительского поведения:

|index|CustomerID|Genre|Age|Income|Score|
|---|---|---|---|---|---|
|0|1|Male|19|15000|0\.39|
|1|2|Male|21|15000|0\.81|
|2|3|Female|20|16000|0\.06|
|3|4|Female|23|16000|0\.77|
|4|5|Female|31|17000|0\.4|

Мы специально выбрали набор данных, содержащий небольшое количество столбцов. На этом примере мы еще сможем проследить логику кластеризации, что было бы значительно сложнее в большом количестве измерений.

Для того, чтобы представлять общую форму датасета давайте изобразим его на графике. В датасете четыре значащих признака, но один их них - пол - бинарный, что легко можно изобразить на графике цветом. Выберем еще два признака, которые расположим по осям, например:

```py
sns.scatterplot(x='Income' , y='Score',data=df , hue='Genre')
```

Мы видим распределение вдоль этих двух численных признаков:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-1.png?raw=true)

Всегда следует помнить, что в реальности данные более многомерны. В данном случае, мы видим только проекцию на плоскость. У нас есть еще одно численное измерение. Попробуйте самостоятельно изобразить датасет в других проекциях и представьте, как он выглядит в трехмерном пространстве.

Для дальнейшей работы нам понадобится произвести минимальную техническую предобработку данных:

```py
x = df.drop(["CustomerID"], axis=1)
X = pd.get_dummies(x)
```

В данном случае, мы избавились от неинформативного атрибута, и преобразовали категориальный атрибут в численные признаки. Вот как выглядит наш датасет уже в виде матрицы признаков:

|index|Age|Income|Score|Genre\_Female|Genre\_Male|
|---|---|---|---|---|---|
|0|19|15000|0\.39|false|true|
|1|21|15000|0\.81|false|true|
|2|20|16000|0\.06|true|false|
|3|23|16000|0\.77|true|false|
|4|31|17000|0\.4|true|false|

Теперь мы можем приступать к построению различных алгоритмов кластеризации на этих данных.

##### Кластеризация К-средних

Для начала попробуем самый простой и распространенный алгоритм кластеризации - К-средних. Для его использования нужно импортировать соответствующий класс из пакета cluster библиотеки sklearn:

```py
from sklearn.cluster import KMeans

k_means = KMeans(n_clusters=3, random_state=42).fit(X)
```

Обратите внимание, что в целом, работа с алгоритмами обучения без учителя в этой библиотеке ничем не отличается от уже известных нам алгоритмов обучения с учителем. Только метод fit() принимает только один аргумент - матрицу признаков X. 

Также нужно заметить, что алгоритм К-средних принимает один обязательный аргумент в своем конструкторе - желаемое количество кластеров. Алгоритм К-средних всегда разбивает выборку на заранее заданное количество. Для начала попробуем три кластера. Про выбор количества кластеров поговорим позднее.

Прежде чем оценивать качество работы обученного алгоритма, надо получить информацию о том, какую точку к какому кластеру модель отнесла. Эти данные содержатся в свойстве labels\_:

```py
y_kmeans = k_means.labels_
```

Выведите этот массив и посмотрите, как обозначаются метки кластеров. А мы используем его для визуализации разделения выборки на кластеры:

```py
plt.scatter(x.Income, x.Score, c=y_kmeans, s=20, cmap='viridis')
```

Объект KMeans сохраняет еще одну полезную для нас информацию, которую можно использовать для визуализации - позиции центров кластеров.

```py
centers = k_means.cluster_centers_
plt.scatter(centers[:, 1], centers[:, 2], c='black', s=200, alpha=0.5)
```

Обратите внимание, какие индексы мы используем для визуализации. Они соответствуют номерам столбцов в матрице признаков. Поэтому если матрица изменилась, либо вы хотите визуализировать другие оси, индексы нужно будет поменять.

Вот как выглядит итоговая визуализация:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-2.png?raw=true)

Очень просто убедиться, что визуализация построена корректно: центры кластеров соответствуют их очертаниям. В данном случае мы, как и задавали, получили три кластера. Но теперь перед нами встает вопрос, а правильное ли число кластеров мы выбрали? Для того,чтобы принимать решение, какое количество кластеров лучше или хуже, нам нужно воспользоваться метрикой качества. Мы возьмем самую распространенную - WCSS, или инерцию. Она уже рассчитана автоматически и помещена в свойство inertia\_

Воспользуемся методом локтя для выбора оптимального количества кластеров. Для этого обучим модель с разным количеством, скажем, от 1 до 10, и для каждого количества выведем значение WCSS на графике:

```py
wcss = []
for i in range(1,11):
	k_means = KMeans(n_clusters=i,random_state=42)
	k_means.fit(X)
	wcss.append(k_means.inertia_)
plt.plot(range(1,11),wcss)
plt.xticks(range(1,11))
_ = plt.show()
```

Мы видим очень типичную картину: чем больше кластеров, тем меньше значение метрики WSCC. Но в начале она падает очень быстро, а при дальнейшем увеличении количества - начинает все больше выравниваться. Наша задача найти "излом" на этой кривой, то есть такое значение, после которого метрика продолжает падает уже значительно медленнее:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-3.png?raw=true)

На некоторых кривых такой локоть более выражен, другие - более плавные, в отдельных случаях локтей может быть несколько. Метод локтя - не точная наука, а некоторый примерный ориентир. В данном случае, можно видеть, что 4 кажется неплохим выбором. Давайте построим кластеризацию на 4 кластера:

```py
k_means = KMeans(n_clusters=4, random_state=42).fit(X)
```

Также изобразим ее на графике. Мы видим очень сходную картину (и она нас должна насторожить):

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-4.png?raw=true)

"На глаз" разделение данной выборки на 4 части не кажется оптимальным. Даже с учетом того, что мы видим датасет в проекции, кажется что при другом расположении центров кластеры были бы более компактными. Это странное поведение алгоритма. Давайте проверим его еще раз, значительно увеличив количество кластеров:

```py
k_means = KMeans(n_clusters=8, random_state=42).fit(X)
```

Мы видим еще более явно странную картину:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-5.png?raw=true)

Центры кластеров подозрительно всегда располагаются вдоль одной оси. И это не кажется оптимальным, когда мы смотрим на общее расположение точек. Если вы видите такое поведение, когда странное поведение выражается вдоль определенной оси, это должно вас навести на мысль о том, что что-то не так с масштабом. 

И действительно, если мы взглянем на подписи осей, станет очевидным, что горизонтальная ось выражена в десятках тысяч, а вертикальная - в долях единицы. У нас типично ненормализованные данные. Давайте исправим эту ошибку и посмотрим, как это отразится на результатах кластеризации.

##### Нормализация признаков

Алгоритм К-средних, будучи метрическим методом, основывается на вычислении расстояний между точками выборки. Поэтому данные перед началом обучения обязательно надо нормировать. Мы воспользуемся стандартизацией, хотя здесь подошел бы и минимаксный способ.

```py
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(X)
X_scaled = scaler.transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
X_scaled.head()
```

Обратите внимание, что мы во избежании путаницы сохраняем датасет в новую переменную. Вот как будет выглядеть нормированная матрица признаков:

|index|Age|Income|Score|Genre\_Female|Genre\_Male|
|---|---|---|---|---|---|
|0|-1\.4245687900521393|-1\.7389991930659485|-0\.43480147996914803|-1\.1281521496355325|1\.1281521496355325|
|1|-1\.2810354107017978|-1\.7389991930659485|1\.1957040699151573|-1\.1281521496355325|1\.1281521496355325|
|2|-1\.3528021003769686|-1\.700829763894176|-1\.7159129834496736|0\.8864052604279182|-0\.8864052604279183|
|3|-1\.1375020313514563|-1\.700829763894176|1\.040417827069033|0\.8864052604279182|-0\.8864052604279183|
|4|-0\.5633685139500905|-1\.6626603347224038|-0\.3959799192576169|0\.8864052604279182|-0\.8864052604279183|

Визуально при построении распределения ничего не поменялось, вся разница видна только в масштабе осей:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-6.png?raw=true)

Теперь давайте повторим наш анализ методом локтя. Для наглядности выведем сразу два графика: по нормализованным данным и по исходной матрице:

```py
unscaled, scaled = [], []
for i in range(1,11):
	unscaled.append(KMeans(n_clusters=i,random_state=42).fit(X).inertia_)
	scaled.append(KMeans(n_clusters=i,random_state=42).fit(X_scaled).inertia_)
plt.plot(range(1,11),unscaled)
plt.plot(range(1,11),scaled)
plt.xticks(range(1,11))
_ = plt.show()
```

Мы видим, что второй график выглядит горизонтальной линией на нулевом уровне:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-7.png?raw=true)

Но все дело опять же в масштабе. Дело в том, что сумма квадратов расстояний по ненормализованным данным учитывала большой масштаб одного из признаков (именно поэтому он доминировал при кластеризации). Посмотрите на подпись вертикальной оси - метрика WCSS у нас измеряется числом с 11 нулями. 

Если вывести только график кластеризации по нормированным данным, все станет понятнее:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-8.png?raw=true)

Теперь WCSS измеряется всего в сотнях. Это кстати, значит, что судя только по этой метрике, кластеры получились гораздо компактнее в нормированной матрице. Оно и понятно. По этому графику аналогично можно вывести, что 4 - оптимальное количество кластеров. Повторим обучение модели:

```py
k_means = KMeans(n_clusters=4, random_state=42).fit(X_scaled)
```

На визуализации мы уже видим совершенно другую картину. Очевидно, нормализация данных очень сильно повлияла на работу алгоритма К-средних:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-9.png?raw=true)

Главное, что центры кластеров уже не выстраиваются в линию параллельно одному из измерений. Однако, кластеры кажутся перемешанными. Это происходит от того, что мы смотрим на датасет в проекции. Поэтому при решении реальных задач не всегда можно опираться на визуализацию, а ориентироваться нужно в первую очередь на метрики.

Но для того, чтобы продемонстрировать то, насколько адекватно работает работает алгоритм K-средних, обучим его на плоском датасете, состоящим всего их двух признаков. Для этого удалим из матрицы лишние столбцы, оставив только те, которые выше мы использовали для визуализации:

```py
X_flat = X_scaled.drop(["Age", "Genre_Female", "Genre_Male"], axis=1)
X_flat.head()
```

У нас останется такой усеченный набор данных:

|index|Income|Score|
|---|---|---|
|0|-1\.7389991930659485|-0\.43480147996914803|
|1|-1\.7389991930659485|1\.1957040699151573|
|2|-1\.700829763894176|-1\.7159129834496736|
|3|-1\.700829763894176|1\.040417827069033|
|4|-1\.6626603347224038|-0\.3959799192576169|

Повторим визуализацию для анализа методом локтя:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-10.png?raw=true)

Мы видим, что довольно отчетливо оптимальное количество кластеров уже 5, а не 4 как ранее. Каждый раз, когда форма распределения точек в пространстве признаков меняется, это может отразиться на их группировке, а значит, метод локтя нужно повторять. Построим кластеризацию по 5 точкам:

```py
k_means = KMeans(n_clusters=5, random_state=42).fit(X_flat)
```

Визуализировав полученную модель, мы получаем довольно ожидаемую картину, когда алгоритм объединил в кластеры сгущения точек, которые хорошо просматриваются на графике:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-11.png?raw=true)

Самостоятельно постройте модель с другими вариантами количества кластеров и посмотрите, как они располагаются на точках. Постройте модель на других признаках для сравнения.

##### Иерархическая кластеризация

Алгоритм К-средних работает вполне удовлетворительно на тех данных, которые мы анализировали в предыдущей части. Но есть и другие алгоритмы кластеризации, которые имеют свои особенности. Одним тех, которые часто требуются является алгоритм агломеративной кластеризации. Его особенность в том, что это иерархический метод. Он позволяет не только разбить выборку на определенное число кластеров, но и построить таксономию - то есть последовательное объединение кластеров также по принципу сходства. 

Работа с моделью агломеративной кластеризации по интерфейсу полностью аналогична:

```py
from sklearn.cluster import AgglomerativeClustering

model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(X_scaled)
```

При создании модели ей передается два аргумента, причем они работают альтернативно. Вы можете задать определенное количество кластеров, тогда порог игнорируется (задается как None), но так мы уже делали. Поэтому воспользуемся вторым "режимом" работы - зададим порог функции расстояния, тогда количество кластеров алгоритм будет выбирать сам и нам нужно задать в этом параметре None.

Для того, чтобы понять, как работает иерархическая кластеризация, мы построим ее визуализацию в виде дерева. Оно покажет как именно, в каком порядке кластеры объединяются. Такая диаграмма называется дендрограммой, и для ее построения существует специальная функция в библиотеке scipy:

```py
from scipy.cluster.hierarchy import dendrogram
```

Для построения дендрограммы в эту функцию нужно передать марицу связности в специальном виде. Мы используем готовый сниппет кода, который принимает на вход объект обученной модели агломерационной кластеризации и строит по нему дендрограмму:

```py
def plot_dendrogram(model, **kwargs):
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    dendrogram(linkage_matrix, **kwargs)
```

Мы приведем этот код без комментариев. Но самостоятельно рекомендуем при его выполнении вывести матрицу связности и разобраться, как она устроена.

```py
plot_dendrogram(model, truncate_mode="level", p=5)
```

Мы получаем следующее визуальное представление работы обученного алгоритма иерархической кластеризации:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-12.png?raw=true)

Разберемся, как устроен это график. По вертикали на нем отложена мера расстояния между объектами или кластерами. По горизонтали - объекты датасета. Соответственно, на уровне самих объектов расстояние между ними равно нулю (расстояние от объекта до самого себя). Ближайшие объекты объединяются в группу, что отражается на дендрограмме характерной формой с перемычкой на том вертикальном уровне, который соответствует расстоянию между объектами. Затем эти группы объединяются между собой, и так до тех пор, пока вся выборка не будет объединена.

Соответственно, интерес представляет именно порядок, в котором разные объекты будут объединяться. Чем две точки выборки дальше друг от друга, тем позже они попадут в один кластер, и тем выше на дендрограмме будет это объединение. 

Соответственно, мы можем как бы "обрезать" это дендрограмму на определенном уровне. Тогда точки, которые попали в одну группу будут "зачислены" в общий кластер. Чем ниже мы обрезаем дендрограмму, тем мельче будут кластеры, но больше их количество. Именно за этот уровень и отвечает аргумент distance_threshold конструктора класса агломеративной кластеризации. 

Либо, мы можем исходить, как раньше, из желаемого количества кластеров. Тогда алгоритм "обрежет" дендрограмму на таком уровне, чтобы получилось необходимое количество групп. Построим кластеризацию на нашем нормализованном датасете:

```py
agg_clustering = AgglomerativeClustering(n_clusters=5).fit(X_scaled)
y_agg = agg_clustering.labels_

plt.scatter(X_flat.Income, X_flat.Score, c=y_agg, s=20, cmap='viridis')
```

Мы получаем картину, очень похожую на результат работы алгоритма К-средних. Самостоятельно сравните результаты работы двух этих алгоритмов по метрикам и визуализируйте в разных проекциях

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-13.png?raw=true)

Для наглядности, обучим этот алгоритм на плоских данных, чтобы показать визуально, как он объединяет точки на плоскости.

```py
agg_clustering = AgglomerativeClustering(n_clusters=4).fit(X_flat)
y_agg = agg_clustering.labels_

plt.scatter(X_flat.Income, X_flat.Score, c=y_agg, s=20, cmap='viridis')
```

Результат похож, но не полностью идентичен алгоритму К-средних:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-14.png?raw=true)

Поэкспериментируйте с этими данными. Изменяйте параметры количества кластеров и порога расстояния, чтобы понять, как именно они влияют на результат.

##### DBSCAN

Еще один алгоритм кластеризации, который необходимо разобрать на практике, потому, что он использует совершенно другой подход - DBSCAN. Он оценивает плотность расположения точек в окрестностях заданной. Попробуем построить его на тех же данных (для наглядности, будем использовать плоские данные):

```py
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.4, min_samples=3).fit(X_flat)
y_db = db.labels_
```

У конструктора данного класса два обязательных аргумента: eps - максимальный радиус вокруг точки, попадание в который считается соседством; и min_samples - количество соседних точек, которых достаточно для признания данной точки внутренней. Оба параметра оказывают существенное влияние на результат кластеризации. Они подбираются эмпирически.

Обычно, эмпирический выбора гиперпараметров модели обучения опирается на значение метрики качества. Так мы делали в предыдущем пункте, в методе локтя. Но сейчас покажем другой подход. Можно мониторить количество кластеров и точек шума, которые определяет алгоритм при данных значениях гиперпараметров. Эти данные можно вычислять так:

```py
n_clusters_ = len(set(y_db)) - (1 if -1 in y_db else 0)
n_noise_ = list(y_db).count(-1)
```

Дело в том, что алгоритм DBSCAN в sklearn использует значение "-1" как метку, показывающую, что данная точка отнесена к шуму. Теперь мы можем построить график зависимости количества кластеров и шума в зависимости от значения гиперпараметра eps:

```py
clusters, noise = [], []
for i in np.logspace(-1, 3, 20):
  db = DBSCAN(eps=i, min_samples=3).fit(X_flat)
  y_db = db.labels_

  clusters.append(len(set(y_db)) - (1 if -1 in y_db else 0))
  noise.append(list(y_db).count(-1))

plt.plot(np.logspace(-1, 3, 20),clusters)
plt.plot(np.logspace(-1, 3, 20),noise)
plt.xscale('log')
_ = plt.show()
```

Мы получаем такой график:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-16.png?raw=true)

Он показывает, что чем больше радиус, тем меньше и кластеров и шума получается. Это логично, ведь при большем радиусе алгоритм будет склонен зачислять все точки в один кластер. Мы можем выбрать оптимальное значение eps опять же, методом локтя, или руководствуясь желаемым количеством кластеров.

Количество точек, то есть параметр min_samples тоже можно подобрать перебором. Его обычно берут в диапазон от 2 до 5, в зависимости от желаемого результата. Давайте построим кластеризацию с подобранными значениями параметров:

```py
db = DBSCAN(eps=0.4, min_samples=3).fit(X_flat)
y_db = db.labels_

plt.scatter(X.Income, X.Score, c=y_db, s=20, cmap='viridis')
```

Самостоятельно изменяйте значения параметров и отследите, как каждый из них влияет на получившуюся кластеризацию. А с данными значениями получается такая картина:

![](https://github.com/koroteevmv/ML_course/blob/main/ML6.1%20clustering/img/ml61-15.png?raw=true)

И здесь уже результаты сильно отличаются от предыдущих двух алгоритмов. Дело в форме расположения точек. Предыдущие две модели выделяли пять очень четких кластеров. Но между ними существуют перемычки, они почти касаются друг друга. Для алгоритмов, оценивающих среднее расстояние между кластерами это не является проблемой. Но для DBSCAN кластеры могут быть "заразными". Зато этот алгоритм может правильно распознавать кластеры сложной формы, сильно отличающиеся от округлых, и отличающиеся друг от друга по размеру. С такими распределениями проблемы уже будут у алгоритмов типа К-средних.


#### Задания для самостоятельного выполнения

1. Для придания смысла кластерам выведите примеры точек выборки для каждого кластера. Сделайте вывод, чем они отличаются.
1. Используйте для визуализации результатов кластеризации другие пары признаков. Сделайте вывод о зависимости кластеризации от признаков.
1. Повторите весь анализ для другого датасета - кластеризации кредитных карт.
1. В задании на иерархическую кластеризацию используйте разные методы расчета расстояния между кластерами. Сделайте выводы.
1. В задании на DBSCAN вычислите метрику WCSS и соотнесите результаты кластеризации через DBSCAN и К-средних.
1. Постройте кривые WCSS в зависимости от параметров DBSCAN.

#### Контрольные вопросы

1. Как работает метод кластеризации К-средних? Когда его стоит применять, а когда он бесполезен?
1. Зачем нужна нормализация признаков перед кластеризацией? Какие методы нормализации можно применять?
1. Какие методы определения расстояния между кластерами (linkage) существуют и чем они отличаются?
1. Что такое дендрограмма, как она устроена и что показывает?
1. Почему метод DBSCAN не очень адекватно работает на исходном датасете в лабораторной работе? Какие особенности датасета определяют выбор предпочтительного метода кластеризации?

#### Дополнительные задания

1. Изучите и используйте для выбора количества кластеров метрику силуэт кластера (silhouette score) и другие метрики качества.
1. Повторите анализ на другом датасете. Найдите датасет с осмысленными названиями точек данных, чтобы можно было вручную контролировать качество и смысл кластеризации.
1. При построении иерархической кластеризации используйте другие метрики расстояния между кластерами и сравните результаты кластеризации и соответствующие им дендрограммы.
1. Ознакомьтесь и примените другие методы кластеризации: HDBSCAN, OPTICS, BIRCH. В чем их особенности и сферы применения?
